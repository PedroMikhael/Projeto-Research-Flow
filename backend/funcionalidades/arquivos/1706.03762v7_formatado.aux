\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Model Architecture}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Transformer - model architecture.}}{4}{}\protected@file@percent }
\newlabel{fig:transformer_architecture}{{3}{4}{}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Encoder and Decoder Stacks}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Attention}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.}}{5}{}\protected@file@percent }
\newlabel{fig:attention_mechanisms}{{3.2}{5}{}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Scaled Dot-Product Attention}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Multi-Head Attention}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Applications of Attention in our Model}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Position-wise Feed-Forward Networks}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Embeddings and Softmax}{7}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention.}}{7}{}\protected@file@percent }
\newlabel{tab:layer_complexity}{{1}{7}{}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Positional Encoding}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Why Self-Attention}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Training}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Training Data and Batching}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Hardware and Schedule}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Optimizer}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Regularization}{10}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.}}{10}{}\protected@file@percent }
\newlabel{tab:bleu_scores}{{2}{10}{}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Machine Translation}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Model Variations}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}English Constituency Parsing}{11}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.}}{12}{}\protected@file@percent }
\newlabel{tab:model_variations}{{3}{12}{}{table.3}{}}
\newlabel{LastPage}{{4}{12}{}{table.4}{}}
\gdef\lastpage@lastpage{12}
\gdef\lastpage@lastpageHy{}
\gdef \@abspage@last{12}
